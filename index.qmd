---
title: "How to use the Great Lakes High Performance Cluster"
author:
  - Katrina S. Munsterman
  - Maximilian H.K. Hesselbarth
date: "`r Sys.Date()`"
format: html
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = FALSE)
```

<!-- badges: start -->

:::{.column-page}

[![License: GPL v3](https://shields.io/badge/License-GPLv3-ad2317?style=for-the-badge)](https://www.gnu.org/licenses/gpl-3.0)
[![Github](https://shields.io/badge/Github-Source%20Code-181717?style=for-the-badge&logo=github)](https://github.com/Allgeier-Lab/great-lakes-hpc)
[![Twitter](https://shields.io/badge/Twitter-@AllgeierLab-489ae9?style=for-the-badge&logo=twitter)](https://twitter.com/AllgeierLab)
[![Homepage](https://shields.io/badge/Homepage-www.jacoballgeier.com-f7cc46?style=for-the-badge&logo=internetexplorer)](https://www.jacoballgeier.com)

:::

<!-- badges: end -->

**All software on the HPC was recently updated (Date: 2022/08/10). So you might need to update some paths and installed software and modules**

## Get accsess to HPC

First you need to get access to the HPC. For this, use [this link](https://teamdynamix.umich.edu/TDClient/30/Portal/Requests/ServiceDet?ID=42) and complete the form to enable your U-Mich account to login to the HPC.

Also, talk to your PI about which funding to use. They can additionally send an e-mail to the support team to add your account to the [U-M Research Computing Package](https://arc.umich.edu/umrcp/) which allows to use a certain amount of free-of-cost minutes. Maybe you are even eligible for your own free-of-costs minutes (ask the GreatLakes support people to find out!).

Once you have an account, use your _Terminal.app_ (on macOS) to login to the HPC. For this use the following line. When asked to type your password, the cursor will not move! Make sure to have your 2-factor authentication ready (_DuoMobile_). In case you are not on Campus, use a [VPN](https://its.umich.edu/enterprise/wifi-networks/vpn/getting-started).

```{bash ssh}
ssh <username>@greatlakes.arc-ts.umich.edu
```

## Setup your HPC account

If you use the HPC for the first time, you need to setup a few files and settings to make future uses easier. Luckily, you have to do these steps only once!

First, check out your `.bash_profile` file by typing `nano .bash_profile`. This will either open the file or create the file if it doesnt exit yet (using a text editor). Copy the following text into the file. To exit the text editor, press `control + x` and agree to saving the file by following the prompts at the bottom of the screen. Now, each time you log into the HPC, it automatically checks if the listed files (`.bash_aliases`, `.bash_modules`) exist and if so load them.

```{bash profile}
# .bash_profile

# load aliases
if [ -f ~/.bash_aliases ]; then
    . ~/.bash_aliases
fi

# load modules
if [ -f ~/.bash_modules ]; then
    . ~/.bash_modules
fi
```

Next, we need to create `.bash_aliases` by typing `nano .bash_aliases`. Copy the following text into the file and exit/save it as previously. This file will create some easy accessible shortcuts/commands. You can always extend this list. So for example, to list all current running jobs submitted from your account you need to type `jobs_run` only instead of a very long original command. Be aware, that the aliases are available **after** you logged out and in to the HPC after creating the file for the first time.

The most important alias are all `jobs_*`.

`jobs_own`: Prints list of all jobs submitted by your account

`jobs_run`: Prints list of all **running** jobs submitted by your account

`jobs_pen`: Prints list of all **pending** jobs submitted by your account

`jobs_n`: Counts all **running** jobs submitted by your account

`jobs_info`: Prints list of all jobs submitted by your account including finished ones

`jobs_finished`: Counts all **finished** jobs submitted by your account

`jobs_kill`: Well...stops all current pending and running jobs submitted by your account

```{bash aliases}
## General HPC/Account information

alias partitions_info='sinfo --sum'
alias jobs_standard='squeue --partition=standard --format="%.12i %.18j %.8u %.6a %.9P %.10l %.10M %.5D %.4C %.7m %16R %.8T"'
alias jobs_largemem='squeue --partition=largemem --format="%.12i %.18j %.8u %.6a  %.9P %.10l %.10M %.5D %.4C %.7m %16R %.8T"'

alias account_info='sacctmgr show assoc user=$USER format=cluster,account,QOS,user,MaxSubmit,MaxJobs,GrpTRES'
alias account_usage='sreport -T billing cluster UserUtilizationByAccount account=jeallg0 Start=2021-01-01 End=$NOW Format=Cluster,Login,Account,TresName,Used'

alias fairshare='sshare -U $USER'

## Information submitted jobs

alias jobs_own='squeue -u $USER --format="%.21i %.18j %.8u %.7a %.9P %.10l %.10M %.5D %.4C %.7m %16R %.8T"'
alias jobs_run='squeue -u $USER --states=RUNNING --format="%.21i %.18j %.8u %.7a %.9P %.10l %.10M %.5D %.4C %.7m %16R %.8T"'
alias jobs_pen='squeue -u $USER --states=PENDING --format="%.21i %.18j %.8u %.7a %.9P %.10l %.10M %.5D %.4C %.7m %16R %.8T"'
alias jobs_n='echo "Running jobs : $(squeue -u $USER --states=RUNNING --noheader | wc -l)"'
alias jobs_kill='scancel -u $USER'
alias jobs_info='sacct -u $USER --units=G --format=JobID,JobName,Account,Partition,Timelimit,Elapsed,AllocNodes,AllocCPU,ReqMem,MaxRSS,State'

alias jobs_finished='echo "Finished jobs: $(find _rslurm* -type f -name "results_*.RDS" | wc -l)"'

## Delete file types

alias rm_logs='rm -rf *.log'
alias rm_future='rm -rf .future/'
alias rm_rds='rm -rf *.rds'
alias rm_rslurm='rm -rf _rslurm_*'

## various

alias reload_bash='source ~/.bash_profile'
alias monitor='htop -u $USER'

alias ll='ls -lFG'

alias R='R --vanilla'
```
Last, you need to create `.bash_modules`. Again, type `nano .bash_modules` and copy the following text before exit/saving the file. This makes sure, each time you use the HPC some pre-installed software libraries are available (such as e.g. `R`). You probably want to load `gcc` and `R`. Depending on the analyses you run, you might need more modules (e.g., `gdal`, `proj`, or `geos`). You can use `module avail` on the HPC to show all options.

```{bash modules}
## Load modules

module load gcc/10.3.0
module load R/4.2.0
```

## Install R packages on the HPC

You need to install all `R` packages that you want to use on the HPC once. For this, login to the HPC and start a `R` session in the _Terminal_ by typing `R` (make sure you loaded the corresponding module). Now, simply run `install.packages(c("package_name_1, package_name_2"))` to install all packages you need. The first time you run the command, it might prompt you a question if you want to create your own libraries folder. Say yes to this. Also, you need to select a CRAN mirror. Just pick any number during the corresponding prompt message. You need to re-do this step each time you want to run some code with a new package you have not installed previously. Once you have installed all packages, exit the `R` session by typing `q()` (...and do not save your workspace image by pressing `n`...).

That's so far everything we need to setup on the HPC! The next steps are on your local disk again.

## rslurm template (can be skipped)

```{bash rslurm_template}
#!/bin/bash

#SBATCH --job-name={{{jobname}}}
#SBATCH --array=0-{{{max_node}}}{{{job_array_task_limit}}}
#SBATCH --cpus-per-task={{{cpus_per_node}}}
{{#flags}}
#SBATCH --{{{name}}}
{{/flags}}
{{#options}}
#SBATCH --{{{name}}}={{{value}}}
{{/options}}
#SBATCH --output={{{jobname}}}.log
#SBATCH --error={{{jobname}}}.log

{{{rscript}}} --vanilla slurm_run.R
```

## Preparing rslurm_template for submission (can be skipped)

(1. make sure that `rslurm_template` is in a folder on your local disk)

(2. for new Rprojects, add `_rslurm_*/` to git ignore, so that git knows not to upload to GitHub)

# Submission to HPC

Prepare your local `R` script (see example below) and run all the code. The `rscript_path` argument should be `"/sw/pkgs/arc/stacks/gcc/10.3.0/R/4.2.0/lib64/R/bin/Rscript"` (Watch out, to adpat this path if you use a different version of `R` on the HPC. You can find out the correct path by login into the HPC, start a R session and run `file.path(R.home("bin"), "Rscript")`). Running the `slurm_apply/map` function will create a `_rslurm_*` folder in your local working directory.

Take this entire folder and use a program such as [_FileZilla_](https://filezilla-project.org) or [_CyberDuck_](https://cyberduck.io) to copy the it to HPC. For example on _FileZilla_, go to `File` > `Site Manager` > and log in to HPC. (Personally, I think CyberDuck is nicer to use...). The fileserver to transfer files is `sftp://greatlakes-xfer.arc-ts.umich.edu`

Now, go back tp the _Terminal_ app, login to the HPC (if not logged in already) and navigate to the just copied `_rslurm_*` folder by typing `cd <foldername>`. Once in that folder, to submit a job to the cluster, type `sbatch submit.sh` in the _Terminal_. This should print a submission message on the console including a unique ID number.

Move back to your home directory by just typing `cd` and check if all jobs are either running, or pending by useing the previously explaine `jobs_*` commands.

# If jobs fail...which they probably will...

If jobs fail with an error (see `jobs_info`), navigate back to the `_rslurm_*` folder using `cd <foldername>` again and list all files using the `ll` command. There should be either one (or several) `*.out` or `*.log` files (e.g. `slurm_1.out` or `error.log`). Print these files on the console by typing `cat <filename>` (e.g., `cat slurm_1.out`) and check why the job failed. Try to fix the issue. Alternativly, you can also download these log files to your computer using _FileZilla_ or _CyberDuck_ and open them with a text editor software.

## Code example

```{r eval = FALSE}
z <- 1.5
fx = function(x, y) (x * 2 + y) ^ z

par_df <- data.frame(x = 1:5)

sjob <- slurm_apply(f = fx, params = par_df, y = 10, jobname = 'my_first_test',
                    nodes = nrow(par_df), cpus_per_node = 1,
                    rscript_path = "/sw/arcts/centos7/stacks/gcc/8.2.0/R/4.1.0/lib64/R/bin/Rscript",
                    slurm_options = list("account" = <Account>,
                                         "partition" = "standard",
                                         "time" = "00:05:00", # hh:mm:ss
                                         "mem-per-cpu" = "7G"),
                    global_objects = c("z"), pkgs = c("purrr", "dplyr"), submit = FALSE)
```

# Completed jobs

If you didn't write directly into your home directory in your cluster function, copy the entire `_rslurm_*` folder to your local disk by replacing the original folder using _FileZilla_ or _CyberDuck_. Then, you can use `get_slurm_out(sjob, "raw")` to collect the results in the same `R` session.

If you wrote the results directly into your home directory, just collect the results yourself by downloading them using _FileZilla_ or _CyberDuck_.

Make sure to delete all `_rslurm_*` folders (on the HPC and locally) once you have all results saved.
