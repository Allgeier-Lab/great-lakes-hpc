---
title: "How to use the Great Lakes High Performance Cluster"
author:
  - Katrina S. Munsterman
  - Maximilian H.K. Hesselbarth
output:
  prettydoc::html_pretty:
    theme: leonids
    highlight: github
date: "`r Sys.Date()`"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = FALSE)
```

<!-- badges: start -->

[![License: GPL v3](https://shields.io/badge/License-GPLv3-ad2317?style=for-the-badge)](https://www.gnu.org/licenses/gpl-3.0)
[![Github](https://shields.io/badge/Github-Source%20Code-181717?style=for-the-badge&logo=github)](https://github.com/Allgeier-Lab/great-lakes-hpc)
[![Twitter](https://shields.io/badge/Twitter-@AllgeierLab-489ae9?style=for-the-badge&logo=twitter)](https://twitter.com/AllgeierLab)
[![Homepage](https://shields.io/badge/Homepage-www.jacoballgeier.com-f7cc46?style=for-the-badge&logo=internetexplorer)](https://www.jacoballgeier.com)

<!-- badges: end -->

## Get accsess to HPC

First you need to get access to the HPC. For this, use [this link](https://teamdynamix.umich.edu/TDClient/30/Portal/Requests/ServiceDet?ID=42) and complete the form to enable your U-Mich account to login to the HPC. 

Also, talk to your PI about which funding to use. They can additionally send an e-mail to the support team to add your account to the [U-M Research Computing Package](https://arc.umich.edu/umrcp/) which allows to use a certain amount of free-of-cost minutes. Maybe you are even eligible for your own free-of-costs minutes.

Once you have an account, use your Terminal.app (on macOS) to login to the HPC. For this use the following line. When asked to type your password, the cursor will not move! Make sure to have your 2-factor authentication ready (DuoMobile). In case you are not on Campus, use a [VPN](https://its.umich.edu/enterprise/wifi-networks/vpn/getting-started).

```{bash ssh}
ssh <username>@greatlakes.arc-ts.umich.edu
```

## Setup your HPC account

If you use the HPC for the first time, you need to setup a few files and settings to make future uses easier. Luckiy, you have to do these steps only once!

First, check out your `.bash_profile` file by typing `nano .bash_profile`. This will either open the file or create the file if it doesnt exit yet (using a text editor). Copy the following text into the file. To exit the text editor, press `control + x` and agree to saving the file by following the prompts. All this does, is each time checking if the listed following files (`.bash_aliases`, `.bash_modules`) exist and if so load them each time you log in.

```{bash profile}
# .bash_profile

# load aliases
if [ -f ~/.bash_aliases ]; then
    . ~/.bash_aliases
fi

# load modules
if [ -f ~/.bash_modules ]; then
    . ~/.bash_modules
fi

## User specific environment and startup programs
# PATH=$PATH:$HOME/.local/bin:$HOME/bin
# export PATH
```

Next, we need to create `.bash_aliases` by typing `nano .bash_aliases`. Copy the following text into the file and exit/save it as previously. This file will create some easy accessible shortcuts/commands. You can always extend this list. So for example, to list all current running jobs submitted from your account you need to type `jobs_run` only instead of a very long original command. Be aware, that the aliases are available **after** you logged out and in to the HPC.

```{bash aliases}
## General HPC/Account information
alias partitions_info='sinfo --sum'
alias jobs_standard='squeue --partition=standard --format="%.12i %.18j %.8u %.6a %.9P %.10l %.10M %.5D %.4C %.7m %16R %.8T"'
alias jobs_largemem='squeue --partition=largemem --format="%.12i %.18j %.8u %.6a  %.9P %.10l %.10M %.5D %.4C %.7m %16R %.8T"'
alias account_info='sacctmgr show assoc user=$USER format=cluster,account,QOS,user,MaxSubmit,MaxJobs,GrpTRES'

alias fairshare='sshare -U $USER'

## Information submitted jobs
alias jobs_own='squeue -u $USER --format="%.21i %.18j %.8u %.7a %.9P %.10l %.10M %.5D %.4C %.7m %16R %.8T"'
alias jobs_run='squeue -u $USER --states=RUNNING --format="%.21i %.18j %.8u %.7a %.9P %.10l %.10M %.5D %.4C %.7m %16R %.8T"'
alias jobs_pen='squeue -u $USER --states=PENDING --format="%.21i %.18j %.8u %.7a %.9P %.10l %.10M %.5D %.4C %.7m %16R %.8T"'
alias jobs_n='squeue -u $USER --states=RUNNING --noheader | wc -l'
alias jobs_kill='scancel -u $USER'
alias jobs_info='sacct -u $USER --units=G --format=JobID,JobName,Account,Partition,Timelimit,Elapsed,AllocNodes,AllocCPU,ReqMem,MaxRSS,State'

## Delete file types
alias rm_logs='rm -rf *.log'
alias rm_rds='rm -rf *.rds'
alias rm_rslurm='rm -rf _rslurm_*'
```
Last, you need to create `.bash_modules`. Again, type `nano .bash_modules` and copy the following text before exit/saving the file. This makes sure, each time you use the HPC some pre-installed software libraries are available (such as e.g. `R`). If you dont use any spatial packages you could delete the lines loading `gdal`, `proj`, and `geos`. If you need any other additional modules, just add them to this file. Similar the the aliases, these changes will take effect once you logged out and in again after creating the file.

```{bash modules}
## Load modules

module load gcc/8.2.0
module load R/4.1.0

module load gdal
module load proj
module load geos
```

## Install R packages on the HPC

You need to install all `R` packages that you want to use on the HPC once. For this, login to the HPC and start a `R` session in the terminal by typing `R` (make sure you loaded the corresponding module). Now, simply run `install.packages(c("package_name_1, package_name_2"))` to install all packages you need. The first time you run the command, it might prompt you a question if you want to create your own libraries folder. Say yes to this. Also, you need to select a CRAN mirror. Just pick any number during the corresponding prompt message. You need to re-do this step each time you want to run some code with a new package you have not installed previously. Once you have installed all packages, exit the `R` session by typing `Q()` (...and do not save your workspace image by pressing `n`...).

Thats so far everything we need to setup on the HPC! The next steps are on your local disk again.

## rslurm template

```{bash rslurm_template}
#!/bin/sh
#SBATCH --account={{ account | jeallg1 }} # account
#SBATCH --job-name={{ job_name }} # job name
#SBATCH --array=1-{{ n_jobs }} # number of processes
#SBATCH --partition={{ partition | standard }} # name of queue
#SBATCH --nodes={{ nodes | 1 }} # if one load, is out on one node
#SBATCH --cpus-per-task={{ n_cpu | 1 }} # set cores per task
#SBATCH --mem-per-cpu={{ mem_cpu | 1024 }} # memory per cpu 
#SBATCH --time={{ walltime | 12:00:00 }} # walltime in hh:mm:ss
#SBATCH --output={{ log_file | /dev/null }}
#SBATCH --error={{ log_file | /dev/null }}

module load gcc/8.2.0
module load R/4.0.2

ulimit -v $(( 1024 * {{ mem_cpu | 1024 }} ))
CMQ_AUTH={{ auth }} R --no-save --no-restore -e 'clustermq:::worker("{{ master }}")'
```

## Code example
